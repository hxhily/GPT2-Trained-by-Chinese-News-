# GPT2 中文新闻语料训练与生成

本项目基于 **GPT-2**，使用中文新闻语料进行训练与文本生成实验。通过对大规模中文新闻数据进行预训练和微调，模型能够生成相对连贯的新闻短段落。

---

## 📌 项目特点

- 使用 **中文新闻语料**（约 279MB），覆盖多领域新闻文本。  
- 基于 **GPT-2 小模型配置**，在单卡型号为GTX4060TI 16G的GPU上训练。  
- 已实现从 **数据预处理 → 模型训练 → 文本生成** 的完整流程。  
- 训练耗时约 **33 小时**。  

---

## ⚙️ 模型配置

训练使用的 GPT2 配置如下：

```json
{
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 10,
  "n_positions": 1024,
  "vocab_size": 13317
}
⚙️ 参数解读
📌 基础超参数

initializer_range: 0.02

模型权重初始化范围，默认 0.02。

控制参数初始值的分布，影响训练稳定性。一般不用改。

layer_norm_epsilon: 1e-05

LayerNorm 中的微小常数，避免除零错误。

默认 1e-5 就行。

📌 模型结构

n_ctx: 1024

模型能处理的最大序列长度（上下文窗口）。

意味着一次能看到 1024 个 token。

中文 tokenizer 后一句话大概 20~50 个 token，1024 token 相当于 2~4 页 A4 纸的文本。

越大显存需求越高（O(n²) 注意力计算）。

在 4060Ti 16G 上，1024 是比较合理的选择。

n_embd: 768

每个 token 的 embedding 维度，同时也是 Transformer 隐层大小。

越大表示模型表达能力越强，但计算量也更大。

GPT2-base 是 768，这里也用 768，属于中等配置。

n_head: 12

多头注意力的头数。

n_embd / n_head = 64，这是比较标准的配置。

增加 head 数会提升捕捉不同语义关系的能力，但也会增加显存占用。

n_layer: 10

Transformer Block 的层数。

GPT2-base 是 12 层，你这里用 10 层，比 base 略小，显存压力更轻一些。

在 16G 显存的显卡上，可以支撑 batch size 8 训练没问题。

n_positions: 1024

位置编码的最大长度，通常与 n_ctx 保持一致。

保证模型能利用 1024 token 的位置信息。
